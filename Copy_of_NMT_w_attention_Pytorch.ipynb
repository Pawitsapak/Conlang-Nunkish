{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NMT w/ attention Pytorch",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "843NabYbO84q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvpje10Z4men",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f1ab3454-f823-4309-a28f-a3bafabb149e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtKcMwC9sisA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "68d6ab8c-41da-409d-8982-b8ee3e892f53"
      },
      "source": [
        "!pip install bcolz\n",
        " \n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import bcolz  # to process the data from Glove File \n",
        "import pickle # to dump and load pretrained glove vectors \n",
        "import copy   # to make deepcopy of python lists and dictionaries\n",
        "import operator\n",
        "import numpy as np\n",
        "from pandas import DataFrame # to visualize the glove word embeddings in form of Dat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.18.4)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp36-cp36m-linux_x86_64.whl size=2656151 sha256=037442c8047e51ab53a813c9c83ed3127a735050f3588423dee8f93ce3473804\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REF09q7GsjBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = bcolz.open(f'/content/drive/My Drive/glove.6B/6B.300.dat')[:]\n",
        "words = pickle.load(open(f'/content/drive/My Drive/glove.6B/6B.300_words.pkl', 'rb'))\n",
        "word2idx = pickle.load(open(f'/content/drive/My Drive/glove.6B/6B.300_idx.pkl', 'rb'))\n",
        " \n",
        "glove = {w: vectors[word2idx[w]] for w in words}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcBDflffsjET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "aa05f9b0-9276-45a7-f157-ad752b934a7c"
      },
      "source": [
        "glove_dframe = DataFrame(vectors, columns=range(1,51), index=words)\n",
        "glove_dframe[100:110]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1653\u001b[0m                 blocks = [\n\u001b[0;32m-> 1654\u001b[0;31m                     \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m                 ]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   3046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3047\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    124\u001b[0m             raise ValueError(\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 300, placement implies 50",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-909e56a1ed9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglove_dframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mglove_dframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1664\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblock_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (400000, 300), indices imply (400000, 50)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbte6L-cspcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sos_index = word2idx['sos']\n",
        "eos_index = word2idx['eos']\n",
        "sos_swap_word = words[0]\n",
        "eos_swap_word = words[1]\n",
        " \n",
        "words[0], words[sos_index] = words[sos_index], words[0]\n",
        "words[1], words[eos_index] = words[eos_index], words[1]\n",
        "word2idx[sos_swap_word], word2idx['sos'] = word2idx['sos'], word2idx[sos_swap_word]\n",
        "word2idx[eos_swap_word], word2idx['eos'] = word2idx['eos'], word2idx[eos_swap_word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUnKzKwDssf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThwlsEp_swAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputLang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}\n",
        "        self.word2count = { word : 1 for word in words }\n",
        "        self.index2word = { i : word for word, i in word2idx.items() }\n",
        "        self.n_words = 400001\n",
        " \n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        " \n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92FQ4TjuswPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        " \n",
        " \n",
        "class OutputLang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        " \n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        " \n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMOz9QG8Xqvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpdI8LFxXqy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?',;:])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?1234567890\\-',;:]]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwjbrwX7Xq1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/content/drive/My Drive/nunkish seq2seq/%s-%s_new.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        pairs = [list((p)) for p in pairs]\n",
        "        input_lang = InputLang(lang1)\n",
        "        output_lang = OutputLang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2GZ1rE0Xq3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 15\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH \n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS2BF6c4Xq44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(pairs[0])\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvBKV4YFXq6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "81ab4a42-619c-4d34-dfbe-f3730f943a9a"
      },
      "source": [
        "input_lang, output_lang, pairs = prepareData('eng', 'nun', False)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 166998 sentence pairs\n",
            "['i am working .', 'nun goqui ikarikoha .']\n",
            "Trimmed to 30482 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 404474\n",
            "nun 59245\n",
            "['this transformation in the relation of forces has its source in objective processes .', 'inhu yurhiruqin akugiq okmuhhu pukkup , makuniqui niruqgammerriq hunnahuisu paquhhuir renhaqquha .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDCrUt8rtdQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix_len = input_lang.n_words\n",
        " \n",
        "weights_matrix = np.zeros((matrix_len, 300))\n",
        "words_found = 0\n",
        " \n",
        "for i, word in enumerate(input_lang.word2index):\n",
        "    try: \n",
        "        weights_matrix[i] = glove[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(300, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2alEFw9tgdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = embedding_dim\n",
        " \n",
        "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        " \n",
        "# Note here we are loading the embedding layer with pretrained weights i.e weight matrix created \n",
        "# from glove vectors --^\n",
        " \n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        " \n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        " \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiRT31yOtiZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        " \n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        " \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        " \n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        " \n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        " \n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        " \n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        " \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N89gW8o9Xq8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojfj8sY4Xq-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WclDXJydXrAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esJWw_c_g1ZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzxKUyQFg1br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYuMGumpg1dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbSUy2wZtupm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, save_every = 10000, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "     \n",
        " \n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        " \n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        " \n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "         \n",
        " \n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "        \n",
        "        if iter % save_every == 0:\n",
        "\n",
        "            print('saving checkpoint {0}'.format(iter))\n",
        "\n",
        "            torch.save({\n",
        "            'iter': iter,\n",
        "            'model_state_dict': encoder.state_dict(),\n",
        "            'optimizer_state_dict': encoder_optimizer.state_dict(),\n",
        "            'loss': print_loss_avg,\n",
        "            }, '/content/drive/My Drive/nunkish seq2seq/seq2seq update/encoder_15_cp{0}.dict'.format(iter))\n",
        "\n",
        "            torch.save({\n",
        "            'iter': iter,\n",
        "            'model_state_dict': attn_decoder.state_dict(),\n",
        "            'optimizer_state_dict': decoder_optimizer.state_dict(),\n",
        "            'loss': print_loss_avg,\n",
        "            }, '/content/drive/My Drive/nunkish seq2seq/seq2seq update/decoder_15_cp{0}.dict'.format(iter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVmL0dstg99H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "            torch.save(encoder1.state_dict(), '/content/drive/My Drive/nunkish seq2seq/training_checkpoints/encoder_{0}.dict'.format(iter))\n",
        "            torch.save(attn_decoder1.state_dict(), '/content/drive/My Drive/nunkish seq2seq/training_checkpoints/decoder_{0}.dict'.format(iter))\n",
        "\n",
        "            torch.save({\n",
        "            'n_iters': n_iters,\n",
        "            'model_state_dict': encoder1.state_dict(),\n",
        "            'optimizer_state_dict': encoder_optimizer.state_dict(),\n",
        "            'loss': plot_losses,\n",
        "            }, '/content/drive/My Drive/nunkish seq2seq/training_checkpoints/encoder_cp{0}.dict'.format(iter))\n",
        "\n",
        "            torch.save({\n",
        "            'n_iters': n_iters,\n",
        "            'model_state_dict': attn_decoder1.state_dict(),\n",
        "            'optimizer_state_dict': decoder_optimizer.state_dict(),\n",
        "            'loss': plot_losses,\n",
        "            }, '/content/drive/My Drive/nunkish seq2seq/training_checkpoints/decoder_cp{0}.dict'.format(iter))\n",
        "\n",
        "            \n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOoN0Lqng-Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAUMaWK2g-Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJEwwnWBg-EK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypjsoa84g-Gg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adc9602a-2a01-402f-947b-8226f345ad47"
      },
      "source": [
        "hidden_size = 300\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder, attn_decoder, 200000, print_every=2000, save_every = 40000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 26s (- 241m 30s) (2000 1%) 6.8599\n",
            "4m 41s (- 229m 42s) (4000 2%) 6.9493\n",
            "6m 57s (- 224m 49s) (6000 3%) 7.0561\n",
            "9m 13s (- 221m 32s) (8000 4%) 7.0035\n",
            "11m 30s (- 218m 42s) (10000 5%) 7.0419\n",
            "13m 51s (- 217m 2s) (12000 6%) 7.0381\n",
            "16m 10s (- 214m 54s) (14000 7%) 7.0668\n",
            "18m 30s (- 212m 53s) (16000 8%) 6.9688\n",
            "20m 50s (- 210m 45s) (18000 9%) 6.8731\n",
            "23m 11s (- 208m 44s) (20000 10%) 6.8559\n",
            "25m 31s (- 206m 29s) (22000 11%) 6.8238\n",
            "27m 50s (- 204m 9s) (24000 12%) 6.7572\n",
            "30m 11s (- 202m 0s) (26000 13%) 6.7265\n",
            "32m 30s (- 199m 43s) (28000 14%) 6.7695\n",
            "34m 52s (- 197m 35s) (30000 15%) 6.7362\n",
            "37m 12s (- 195m 18s) (32000 16%) 6.6621\n",
            "39m 32s (- 193m 2s) (34000 17%) 6.6007\n",
            "41m 53s (- 190m 50s) (36000 18%) 6.5910\n",
            "44m 14s (- 188m 34s) (38000 19%) 6.5808\n",
            "46m 33s (- 186m 15s) (40000 20%) 6.4910\n",
            "saving checkpoint 40000\n",
            "48m 58s (- 184m 13s) (42000 21%) 6.5361\n",
            "51m 18s (- 181m 54s) (44000 22%) 6.4541\n",
            "53m 38s (- 179m 36s) (46000 23%) 6.4633\n",
            "55m 58s (- 177m 15s) (48000 24%) 6.4411\n",
            "58m 18s (- 174m 55s) (50000 25%) 6.3812\n",
            "60m 39s (- 172m 37s) (52000 26%) 6.3718\n",
            "63m 0s (- 170m 21s) (54000 27%) 6.2655\n",
            "65m 20s (- 168m 1s) (56000 28%) 6.2649\n",
            "67m 41s (- 165m 43s) (58000 28%) 6.2706\n",
            "70m 2s (- 163m 26s) (60000 30%) 6.2176\n",
            "72m 22s (- 161m 6s) (62000 31%) 6.1388\n",
            "74m 43s (- 158m 47s) (64000 32%) 6.1042\n",
            "77m 4s (- 156m 28s) (66000 33%) 6.1102\n",
            "79m 23s (- 154m 6s) (68000 34%) 6.0494\n",
            "81m 42s (- 151m 45s) (70000 35%) 6.0305\n",
            "84m 2s (- 149m 24s) (72000 36%) 5.9706\n",
            "86m 21s (- 147m 2s) (74000 37%) 5.9509\n",
            "88m 41s (- 144m 42s) (76000 38%) 5.9423\n",
            "91m 0s (- 142m 21s) (78000 39%) 5.8655\n",
            "93m 20s (- 140m 1s) (80000 40%) 5.8932\n",
            "saving checkpoint 80000\n",
            "95m 43s (- 137m 45s) (82000 41%) 5.8251\n",
            "98m 3s (- 135m 25s) (84000 42%) 5.8027\n",
            "100m 24s (- 133m 6s) (86000 43%) 5.8074\n",
            "102m 45s (- 130m 47s) (88000 44%) 5.7978\n",
            "105m 6s (- 128m 28s) (90000 45%) 5.7218\n",
            "107m 27s (- 126m 9s) (92000 46%) 5.6923\n",
            "109m 48s (- 123m 49s) (94000 47%) 5.6311\n",
            "112m 9s (- 121m 30s) (96000 48%) 5.5902\n",
            "114m 30s (- 119m 10s) (98000 49%) 5.6011\n",
            "116m 51s (- 116m 51s) (100000 50%) 5.5310\n",
            "119m 10s (- 114m 30s) (102000 51%) 5.5701\n",
            "121m 34s (- 112m 12s) (104000 52%) 5.4918\n",
            "123m 55s (- 109m 53s) (106000 53%) 5.4450\n",
            "126m 16s (- 107m 34s) (108000 54%) 5.4527\n",
            "128m 37s (- 105m 14s) (110000 55%) 5.4383\n",
            "130m 57s (- 102m 54s) (112000 56%) 5.3595\n",
            "133m 19s (- 100m 34s) (114000 56%) 5.3271\n",
            "135m 40s (- 98m 14s) (116000 57%) 5.3263\n",
            "138m 1s (- 95m 54s) (118000 59%) 5.3173\n",
            "140m 22s (- 93m 34s) (120000 60%) 5.2423\n",
            "saving checkpoint 120000\n",
            "142m 45s (- 91m 16s) (122000 61%) 5.1719\n",
            "145m 6s (- 88m 56s) (124000 62%) 5.2274\n",
            "147m 28s (- 86m 36s) (126000 63%) 5.2161\n",
            "149m 48s (- 84m 16s) (128000 64%) 5.1328\n",
            "152m 10s (- 81m 56s) (130000 65%) 5.1283\n",
            "154m 31s (- 79m 36s) (132000 66%) 5.0571\n",
            "156m 54s (- 77m 16s) (134000 67%) 5.1048\n",
            "159m 14s (- 74m 56s) (136000 68%) 5.0004\n",
            "161m 33s (- 72m 35s) (138000 69%) 4.9838\n",
            "163m 54s (- 70m 14s) (140000 70%) 5.0221\n",
            "166m 15s (- 67m 54s) (142000 71%) 4.8940\n",
            "168m 35s (- 65m 33s) (144000 72%) 4.9032\n",
            "170m 56s (- 63m 13s) (146000 73%) 4.8716\n",
            "173m 15s (- 60m 52s) (148000 74%) 4.8504\n",
            "175m 35s (- 58m 31s) (150000 75%) 4.8363\n",
            "177m 55s (- 56m 11s) (152000 76%) 4.7923\n",
            "180m 15s (- 53m 50s) (154000 77%) 4.7868\n",
            "182m 36s (- 51m 30s) (156000 78%) 4.7248\n",
            "184m 55s (- 49m 9s) (158000 79%) 4.7440\n",
            "187m 15s (- 46m 48s) (160000 80%) 4.7109\n",
            "saving checkpoint 160000\n",
            "189m 39s (- 44m 29s) (162000 81%) 4.7787\n",
            "191m 58s (- 42m 8s) (164000 82%) 4.6713\n",
            "194m 18s (- 39m 47s) (166000 83%) 4.7146\n",
            "196m 39s (- 37m 27s) (168000 84%) 4.6411\n",
            "198m 59s (- 35m 6s) (170000 85%) 4.5929\n",
            "201m 19s (- 32m 46s) (172000 86%) 4.5878\n",
            "203m 40s (- 30m 26s) (174000 87%) 4.5278\n",
            "206m 2s (- 28m 5s) (176000 88%) 4.5809\n",
            "208m 23s (- 25m 45s) (178000 89%) 4.5571\n",
            "210m 43s (- 23m 24s) (180000 90%) 4.4690\n",
            "213m 6s (- 21m 4s) (182000 91%) 4.4540\n",
            "215m 28s (- 18m 44s) (184000 92%) 4.4523\n",
            "217m 52s (- 16m 23s) (186000 93%) 4.4830\n",
            "220m 14s (- 14m 3s) (188000 94%) 4.3598\n",
            "222m 35s (- 11m 42s) (190000 95%) 4.4331\n",
            "224m 57s (- 9m 22s) (192000 96%) 4.3585\n",
            "227m 18s (- 7m 1s) (194000 97%) 4.3485\n",
            "229m 40s (- 4m 41s) (196000 98%) 4.2938\n",
            "232m 0s (- 2m 20s) (198000 99%) 4.2868\n",
            "234m 22s (- 0m 0s) (200000 100%) 4.2173\n",
            "saving checkpoint 200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNqtvCaPzAUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(encoder1.state_dict(), '/content/drive/My Drive/nunkish seq2seq/training_checkpoints/encoder.dict')\n",
        "torch.save(attn_decoder1.state_dict(), '/content/drive/My Drive/nunkish seq2seq/training_checkpoints/decoder.dict')\n",
        "\n",
        "#load model\n",
        "#encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "#decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "#encoder.load_state_dict(torch.load('encoder.dict'))\n",
        "#decoder.load_state_dict(torch.load('decoder.dict'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnbO8Qu2DfvM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "41b61518-65bd-4b62-a778-a40b62d5dd57"
      },
      "source": [
        "learning_rate=0.01\n",
        "hidden_size = 512\n",
        "#load checkpoints\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
        "\n",
        "checkpoint = torch.load('/content/drive/My Drive/nunkish seq2seq/seq2seq update/encoder_15_cp200000.dict')\n",
        "encoder.load_state_dict(checkpoint['model_state_dict'])\n",
        "encoder_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "iter = checkpoint['iter']\n",
        "print_loss_avg, = checkpoint['loss']\n",
        "\n",
        "encoder.eval()\n",
        "# - or -\n",
        "encoder.train()\n",
        "\n",
        "###########################################################################################################################\n",
        "\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "decoder_optimizer = optim.SGD(attn_decoder1.parameters(), lr=learning_rate)\n",
        "\n",
        "checkpoint = torch.load('/content/drive/My Drive/nunkish seq2seq/seq2seq update/decoder_15_cp200000.dict')\n",
        "attn_decoder.load_state_dict(checkpoint['model_state_dict'])\n",
        "decoder_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "iter = checkpoint['iter']\n",
        "print_loss_avg, = checkpoint['loss']\n",
        "\n",
        "attn_decoder.eval()\n",
        "# - or -\n",
        "attn_decoder.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttnDecoderRNN(\n",
              "  (embedding): Embedding(322582, 512)\n",
              "  (attn): Linear(in_features=1024, out_features=200, bias=True)\n",
              "  (attn_combine): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (gru): GRU(512, 512)\n",
              "  (out): Linear(in_features=512, out_features=322582, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2ibEmhFiGhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97bcfd43-1fad-4508-d582-2cdb2799459f"
      },
      "source": [
        "hidden_size = 300\n",
        "trainIters(encoder, attn_decoder, 200000, print_every=2000, save_every = 40000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 35s (- 256m 24s) (2000 1%) 4.2827\n",
            "4m 57s (- 242m 47s) (4000 2%) 4.1450\n",
            "7m 20s (- 237m 13s) (6000 3%) 4.2096\n",
            "9m 42s (- 232m 52s) (8000 4%) 4.1405\n",
            "12m 4s (- 229m 17s) (10000 5%) 4.1678\n",
            "14m 25s (- 225m 59s) (12000 6%) 4.1666\n",
            "16m 46s (- 222m 57s) (14000 7%) 4.0897\n",
            "19m 9s (- 220m 14s) (16000 8%) 4.1615\n",
            "21m 30s (- 217m 28s) (18000 9%) 4.1331\n",
            "23m 53s (- 215m 3s) (20000 10%) 4.1041\n",
            "26m 16s (- 212m 32s) (22000 11%) 4.0640\n",
            "28m 38s (- 210m 0s) (24000 12%) 4.0790\n",
            "31m 2s (- 207m 44s) (26000 13%) 4.0671\n",
            "33m 23s (- 205m 8s) (28000 14%) 3.9717\n",
            "35m 45s (- 202m 40s) (30000 15%) 3.9707\n",
            "38m 8s (- 200m 15s) (32000 16%) 3.9407\n",
            "40m 32s (- 197m 54s) (34000 17%) 4.0396\n",
            "42m 54s (- 195m 26s) (36000 18%) 4.0140\n",
            "45m 15s (- 192m 57s) (38000 19%) 4.0187\n",
            "47m 37s (- 190m 29s) (40000 20%) 4.0887\n",
            "saving checkpoint 40000\n",
            "50m 2s (- 188m 15s) (42000 21%) 4.0298\n",
            "52m 24s (- 185m 48s) (44000 22%) 4.0057\n",
            "54m 47s (- 183m 25s) (46000 23%) 3.9770\n",
            "57m 10s (- 181m 1s) (48000 24%) 4.0262\n",
            "59m 32s (- 178m 37s) (50000 25%) 3.9969\n",
            "61m 54s (- 176m 11s) (52000 26%) 3.9809\n",
            "64m 16s (- 173m 46s) (54000 27%) 3.9806\n",
            "66m 39s (- 171m 24s) (56000 28%) 4.0052\n",
            "69m 1s (- 168m 58s) (58000 28%) 3.9499\n",
            "71m 23s (- 166m 35s) (60000 30%) 3.9420\n",
            "73m 46s (- 164m 12s) (62000 31%) 4.0348\n",
            "76m 8s (- 161m 48s) (64000 32%) 3.9854\n",
            "78m 31s (- 159m 24s) (66000 33%) 3.9509\n",
            "80m 53s (- 157m 1s) (68000 34%) 4.0404\n",
            "83m 16s (- 154m 38s) (70000 35%) 4.0341\n",
            "85m 37s (- 152m 12s) (72000 36%) 3.9662\n",
            "87m 59s (- 149m 49s) (74000 37%) 3.9683\n",
            "90m 21s (- 147m 25s) (76000 38%) 4.0603\n",
            "92m 42s (- 145m 0s) (78000 39%) 4.0827\n",
            "95m 1s (- 142m 32s) (80000 40%) 5.2127\n",
            "saving checkpoint 80000\n",
            "97m 26s (- 140m 12s) (82000 41%) 5.2206\n",
            "99m 47s (- 137m 48s) (84000 42%) 5.1750\n",
            "102m 8s (- 135m 24s) (86000 43%) 5.2391\n",
            "104m 29s (- 132m 59s) (88000 44%) 5.1485\n",
            "106m 49s (- 130m 33s) (90000 45%) 5.0755\n",
            "109m 9s (- 128m 8s) (92000 46%) 5.0876\n",
            "111m 29s (- 125m 43s) (94000 47%) 5.0263\n",
            "113m 49s (- 123m 18s) (96000 48%) 5.3339\n",
            "116m 9s (- 120m 53s) (98000 49%) 5.5263\n",
            "118m 28s (- 118m 28s) (100000 50%) 5.4023\n",
            "120m 49s (- 116m 5s) (102000 51%) 5.3369\n",
            "123m 9s (- 113m 40s) (104000 52%) 5.3372\n",
            "125m 28s (- 111m 16s) (106000 53%) 5.4105\n",
            "127m 45s (- 108m 49s) (108000 54%) 5.6917\n",
            "130m 3s (- 106m 24s) (110000 55%) 6.4980\n",
            "132m 20s (- 103m 59s) (112000 56%) 6.4947\n",
            "134m 36s (- 101m 33s) (114000 56%) 6.5908\n",
            "136m 52s (- 99m 7s) (116000 57%) 6.4305\n",
            "139m 8s (- 96m 41s) (118000 59%) 6.5059\n",
            "141m 24s (- 94m 16s) (120000 60%) 7.1237\n",
            "saving checkpoint 120000\n",
            "143m 42s (- 91m 52s) (122000 61%) 6.8955\n",
            "145m 55s (- 89m 26s) (124000 62%) 6.4317\n",
            "148m 8s (- 87m 0s) (126000 63%) 6.3491\n",
            "150m 20s (- 84m 33s) (128000 64%) 6.3049\n",
            "152m 33s (- 82m 8s) (130000 65%) 6.3251\n",
            "154m 47s (- 79m 44s) (132000 66%) 6.6871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b30d88dc593f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-56370b345bed>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, save_every, learning_rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 18\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-77785d753587>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlib2Njlg1gB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "274c922b-3046-428a-bdf8-825de4abadcc"
      },
      "source": [
        "evaluateRandomly(encoder, attn_decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> now they are not allowed to sail more than five miles off shore .\n",
            "= immeqaha ruhuqiq ikanha uinha puiq huqqi pinmihirru unapuhirrummuhaguhiqqui .\n",
            "< ugukruq immeha 100 uqquha pihhuk . . . . . . . . . .\n",
            "\n",
            "> the other was the report that kerrey had been involved in war crimes .\n",
            "= pukkenka rokki sahhur rakkunruqiq ihamuhhuk onku yoshi .\n",
            "< inhu sahhur sahhur yoshi yoshi mek . . . <EOS>\n",
            "\n",
            "> i protested and they punched me in the face .\n",
            "= nun uhukra ohikmmah hokigirruisiq on paruhhiq rahha gihhunuk .\n",
            "< nun , onnui nun nun nun . <EOS>\n",
            "\n",
            "> what is a rajini speech without story ?\n",
            "= rahhiruhui iqquhu kusini moyyu ?\n",
            "< kusini kusini kusini ? ? <EOS>\n",
            "\n",
            "> the judge has postponed hearing till then .\n",
            "= ihunuq hukruqirupuru guqurra giyukunuisui huqqi guihhaqquk nihimuhi .\n",
            "< inhu guqurra guqurra guqurra panma unapuhi . . . . . . . . .\n",
            "\n",
            "> but we have been on the retreat .\n",
            "= unuq mingunruy yossummuhhaqqep .\n",
            "< unuq nunruq nup nunruq . . . . . . . . . . .\n",
            "\n",
            "> many in the tamil film field are showing interest in using this camera .\n",
            "= inhu ropukugui immeha hupiqhhikuisaquriq musunmuhahhu muquk ukgup ruhharinkunuk .\n",
            "< inhu muqu hikuimmuhu muqgoka hikuimmuhu . . . . . . . . . .\n",
            "\n",
            "> then the disciples understood that he spoke to them of john the baptist .\n",
            "= uguk segunznununuirrakihhah hunruqarray yennuk onka yizukruq ummeqaha ukinharenhukruq .\n",
            "< ummeqaha rukhhuk yizukruq yizukruq mukki uguk yennuk . <EOS>\n",
            "\n",
            "> sabapathy 's usual composer yuvan shankar raja composes the music .\n",
            "= yumumuhisin uzhunu iyuisupuimmuquk sagun yunruk kusu muhuhharra iyui .\n",
            "< sagun uzhunu sagun sagun sagun sagun sagun sagun . . . . . . .\n",
            "\n",
            "> in 1994 he was made chairman of the party .\n",
            "= 1994q uguk ruhyisin huquigukunuk .\n",
            "< uguk ruhyisin huquigukunuk . . . . . . . . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq9qB9CMv32Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b9f148c-e07d-4f73-a35f-f8aa0ab2fdae"
      },
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"i love you .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9117e757b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkkGidXTwD42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder, attn_decoder, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbCjAlu7wEhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluateAndShowAttention(\"i love you .\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}